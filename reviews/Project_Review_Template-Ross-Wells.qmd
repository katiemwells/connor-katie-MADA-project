---
title: Project Review Template 
author: Abbie Klinker
date: "`r file.mtime(knitr::current_input())`"
format: 
  html:
    toc: true
    toc-depth: 3
    number-sections: true
---

# Overview

Title of project: COVID-19 Vaccine Popularity by Region in the US

Name of project author(s): Connor Ross & Katie Wells

Name of project reviewer: Abbie Klinker

# Specific project content evaluation
Evaluate the different parts of the project by filling in the sections below.


## Background, Context and Motivation
How well is the context of the project described? Is a comprehensive background, including summary of previous/related work given? Is the project well placed into the context of existing work (including proper referencing of existing work). Is it clear why the project was undertaken and what new information it hopes to provide?

### Feedback and Comments

Good background! Clear and detailed

### Summary assessment (PICK ONE, DELETE THE OTHERS)

* strong contextualization and motivation


## Question description

How well and clear are the question(s)/hypotheses the project aims to address described? Is it clear how the questions relate to the data?


### Feedback and Comments

Clear and concisely outlined in the paper.

### Summary assessment

* question/hypotheses fully clear


## Data description
How well is the data overall described? Is the source provided? Is a codebook or other meta-information available that makes it clear what the data is? 

### Feedback and Comments

Room to elaborate where the data came from (CDC?), but descriptive of what is included and how to apply it to question. 

### Summary assessment

*  *source* and overall structure of data somewhat explained


## Data wrangling and exploratory analysis
How well is the data cleaned/processed and explored? Are all steps reasonable and well explained? Are alternatives discussed and considered? Are meaningful exploratory results shown (e.g. in the supplementary materials)?

### Feedback and Comments

Graph 1 may benefit from a log scale or facet_wrap by vaccine type with independent axes so they can be better distinguished. Included in the paper twice (back to back) 

### Summary assessment

* essentially no weaknesses in wrangling and exploratory component


## Appropriateness of Analysis
Were the analysis methods appropriate for the data? Was the analysis done properly? Were different components of the analysis (e.g. performance measure, variable selection, data pre-processing, model evaluation) done in the best way possible and explained well?

### Feedback and Comments

Could elaborate on why tree model was chosen versus an assessment between several model types. Not an inappropriate analysis, but could provide better rationale. 

### Summary assessment

* strong and reasonable analysis

## Presentation
How well are results presented? Are tables and figures easy to read and understand? Are the main figures/tables publication level quality? 

### Feedback and Comments

Basic analysis tables may benefit from total model metrics, not just with respect to predictors. See comments on graph in exploratory section.

Room to polish output from ML models and elaborate on the significance of the results. 

### Summary assessment

* results are presented ok, with room for improvement


## Discussion/Conclusions
Are the study findings properly discussed? Are strengths and limitations acknowledged? Are findings interpreted properly?

### Feedback and Comments

Good job! 

### Summary assessment

* strong, complete and clear discussion




# Overall project content evaluation
Evaluate overall features of the project  by filling in the sections below.


## Structure
Is the project well structured? Are files in well labeled folders? Do files have reasonable names? Are all "junk" files not needed for analysis/reproduction removed? By just looking at files and folders, can you get an idea of how things fit together?

### Feedback and Comments

Folders easily navigatable for the project

### Summary assessment

* well structured


## Documentation 
How well is the project documented? Are you able to understand each step of the whole analysis, each decision that was made, and each line of code? Is enough information provided as comments in code or as part of Rmd files? 

### Feedback and Comments

Code commented out well, but not much detail or rationale for various steps.

### Summary assessment

* fully and well documented


## Reproducibility
Are all results fully reproducible? Is documentation provided which clearly explains how to reproduce things, and does it work without the need for any manual intervention? Are you able to re-run the whole analysis without having to do manual interventions/edits?

### Feedback and Comments

No packages or data called in exploration file, so the viewer would have already had to have them loaded/can't start from empty environment. This prevents the viewer being able to render the files independently. I had to run individual code chucks from wrangling so the environment transferred over versus independently loading associated packages and data in the exploration.qmd file itself. This could get tedious for longer scripts.

*Also happened in fitting.qmd with SubData file and "see" package

### Summary assessment

* small parts not reproducible or required manual intervention 


## Thoroughness
How thorough was the overall study? Were alternatives (e.g. different ways of processing the data or different models) considered? Were alternatives discussed? Were the questions/hypotheses fully and thoroughly addressed?

### Feedback and Comments

My only suggestion would be to investigate other ML model options

### Summary assessment

* strong level of thoroughness


# Further comments

Good work! I think if you polish/clarify a couple things in the output, this is a great project!




